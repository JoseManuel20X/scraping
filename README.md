# ğŸ“š **Books Web Scraper** ğŸ“š
## AutomatizaciÃ³n de la extracciÃ³n de datos de libros

### ğŸ“– **DescripciÃ³n del Proyecto** ğŸ“–

**Books Web Scraper** es una herramienta automatizada para la extracciÃ³n de datos de libros desde el sitio web **Books to Scrape**. Este proyecto emplea tanto **scraping estÃ¡tico** como **dinÃ¡mico**, y estÃ¡ diseÃ±ado para almacenar la informaciÃ³n extraÃ­da en una base de datos **PostgreSQL**. AdemÃ¡s, ofrece funcionalidades como la descarga automÃ¡tica de imÃ¡genes de portadas y una **API REST** para consultar los datos extraÃ­dos.

### ğŸ” **Â¿QuÃ© Hace?** ğŸ”

- **ExtracciÃ³n automÃ¡tica** de datos de libros: tÃ­tulo, precio, imagen.
- **Almacenamiento persistente** de los datos en **PostgreSQL**.
- **Descarga automÃ¡tica** de imÃ¡genes de las portadas de los libros.
- **API REST** para la consulta y consumo de los datos extraÃ­dos.
- **AutomatizaciÃ³n de tareas** para ejecutar el scraping de manera periÃ³dica.
- **AnÃ¡lisis de precios** y catÃ¡logos de libros.

### ğŸ› ï¸ **TecnologÃ­as Utilizadas** ğŸ› ï¸

#### Backend y Scraping

- **Python 3.8+** - Lenguaje principal del proyecto.
- **Selenium** - Herramienta de automatizaciÃ³n para scraping dinÃ¡mico.
- **BeautifulSoup4** - LibrerÃ­a para parsing HTML en scraping estÃ¡tico.
- **Requests** - LibrerÃ­a para realizar peticiones HTTP.

#### Base de Datos y API

- **PostgreSQL** - Sistema de gestiÃ³n de base de datos relacional.
- **Psycopg2** - Conector de PostgreSQL para Python.
- **Flask** - Framework para la creaciÃ³n de la API REST.
- **Flask-CORS** - GestiÃ³n de CORS en la API.

#### AutomatizaciÃ³n y Utilidades

- **APScheduler** - LibrerÃ­a para la programaciÃ³n de tareas automÃ¡ticas.
- **Python-dotenv** - Manejo de variables de entorno.
- **Logging** - Sistema integrado para registro de eventos.

### ğŸš€ **InstalaciÃ³n y ConfiguraciÃ³n** ğŸš€

#### **Prerrequisitos**

- **Python 3.8+**.
- **PostgreSQL 13+**.
- **Google Chrome** (para scraping dinÃ¡mico).
- **Git**.

#### **Pasos de InstalaciÃ³n**

1. Clonar el repositorio:

    ```bash
    git clone https://github.com/JoseManuel20X/scraping.git
    ```

2. Instalar las dependencias de Python:

    ```bash
    cd scraping
    pip install -r requirements.txt
    ```

3. Configurar las variables de entorno en un archivo `.env`:

    ```bash
    # Archivo .env de ejemplo
    DB_HOST=localhost
    DB_PORT=5432
    DB_NAME=nombre_bd
    DB_USER=usuario
    DB_PASSWORD=contraseÃ±a
    ```

4. Crear y configurar la base de datos en PostgreSQL.

5. Ejecutar el script de scraping:

    ```bash
    python scraper.py
    ```

6. Iniciar la API Flask:

    ```bash
    flask run
    ```

### ğŸ“‘ **Licencia** ğŸ“‘

Este proyecto estÃ¡ bajo la **Licencia MIT**. Ver el archivo [LICENSE](LICENSE) para mÃ¡s detalles.
