# üìö **Books Web Scraper** üìö
## Automatizaci√≥n de la extracci√≥n de datos de libros

### üìñ **Descripci√≥n del Proyecto** üìñ

**Books Web Scraper** es una herramienta automatizada para la extracci√≥n de datos de libros desde el sitio web **Books to Scrape**. Este proyecto emplea tanto **scraping est√°tico** como **din√°mico**, y est√° dise√±ado para almacenar la informaci√≥n extra√≠da en una base de datos **PostgreSQL**. Adem√°s, ofrece funcionalidades como la descarga autom√°tica de im√°genes de portadas y una **API REST** para consultar los datos extra√≠dos.

### üîç **¬øQu√© Hace?** üîç

- **Extracci√≥n autom√°tica** de datos de libros: t√≠tulo, precio, imagen.
- **Almacenamiento persistente** de los datos en **PostgreSQL**.
- **Descarga autom√°tica** de im√°genes de las portadas de los libros.
- **API REST** para la consulta y consumo de los datos extra√≠dos.
- **Automatizaci√≥n de tareas** para ejecutar el scraping de manera peri√≥dica.
- **An√°lisis de precios** y cat√°logos de libros.

### üõ†Ô∏è **Tecnolog√≠as Utilizadas** üõ†Ô∏è

#### Backend y Scraping

- **Python 3.8+** - Lenguaje principal del proyecto.
- **Selenium** - Herramienta de automatizaci√≥n para scraping din√°mico.
- **BeautifulSoup4** - Librer√≠a para parsing HTML en scraping est√°tico.
- **Requests** - Librer√≠a para realizar peticiones HTTP.

#### Base de Datos y API

- **PostgreSQL** - Sistema de gesti√≥n de base de datos relacional.
- **Psycopg2** - Conector de PostgreSQL para Python.
- **Flask** - Framework para la creaci√≥n de la API REST.
- **Flask-CORS** - Gesti√≥n de CORS en la API.

#### Automatizaci√≥n y Utilidades

- **APScheduler** - Librer√≠a para la programaci√≥n de tareas autom√°ticas.
- **Python-dotenv** - Manejo de variables de entorno.
- **Logging** - Sistema integrado para registro de eventos.

### üöÄ **Instalaci√≥n y Configuraci√≥n** üöÄ

#### **Prerrequisitos**

- **Python 3.8+**.
- **PostgreSQL 13+**.
- **Google Chrome** (para scraping din√°mico).
- **Git**.

#### **Pasos de Instalaci√≥n**

1. Clonar el repositorio:

    ```bash
    git clone https://github.com/JoseManuel20X/scraping.git
    ```

2. Instalar las dependencias de Python:

    ```bash
    cd scraping
    pip install -r requirements.txt
    ```

3. Configurar las variables de entorno en un archivo `.env`:

    ```bash
    # Archivo .env de ejemplo
    DB_HOST=localhost
    DB_PORT=5432
    DB_NAME=nombre_bd
    DB_USER=usuario
    DB_PASSWORD=contrase√±a
    ```

4. **Configurar la base de datos**:

    Aseg√∫rate de tener una base de datos PostgreSQL configurada y creada. El proyecto intentar√° crear la base de datos y las tablas si no existen.

5. **Ejecutar el scraper din√°mico**:

    ```bash
    python scraper_dynamic.py
    ```

6. **Ejecutar el scraper est√°tico**:

    ```bash
    python scraper_static.py
    ```

7. **Ejecutar el scheduler**:

    El scheduler ejecutar√° ambos scrapers cada 30 minutos. Para iniciar el scheduler:

    ```bash
    python scheduler.py
    ```

8. **Ejecutar el servidor de API**:

    Para iniciar la API REST:

    ```bash
    python json_api_server.py
    ```

    La API estar√° disponible en `http://localhost:5000`.
