Un sistema completo de web scraping para extraer informaci√≥n de libros de Books to Scrape, con almacenamiento en PostgreSQL y API REST para consulta de datos.

üöÄ Caracter√≠sticas

Scraping Est√°tico: Extracci√≥n r√°pida usando requests y BeautifulSoup
Scraping Din√°mico: Navegaci√≥n completa con Selenium para contenido JavaScript
Descarga de Im√°genes: Almacenamiento autom√°tico de portadas de libros
Base de Datos: Persistencia en PostgreSQL con estructura normalizada
API REST: Endpoints para consultar datos extra√≠dos
Programaci√≥n Autom√°tica: Ejecuci√≥n peri√≥dica con APScheduler
Logging Completo: Registro detallado de operaciones y errores
Exportaci√≥n JSON: Respaldo de datos en formato JSON

üõ†Ô∏è Tecnolog√≠as Utilizadas
Tecnolog√≠aVersi√≥nProp√≥sitoPython3.8+Lenguaje principalPostgreSQL13+Base de datosSelenium4.0+Scraping din√°micoBeautifulSoup44.9+Parsing HTMLFlask2.0+API RESTAPScheduler3.8+Programaci√≥n de tareasRequests2.25+HTTP requestsPsycopg22.8+Conector PostgreSQL
üì¶ Instalaci√≥n
Prerrequisitos

Python 3.8 o superior
PostgreSQL 13 o superior
Google Chrome (para scraping din√°mico)
ChromeDriver

1. Clonar el repositorio
bashgit clone https://github.com/tu-usuario/books-web-scraper.git
cd books-web-scraper
2. Crear entorno virtual
bashpython -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate
3. Instalar dependencias
bashpip install -r requirements.txt
4. Configurar variables de entorno
Crear archivo .env en la ra√≠z del proyecto:
env# Configuraci√≥n de Base de Datos
DB_HOST=localhost
DB_PORT=5432
DB_NAME=books_scraper
DB_USER=tu_usuario
DB_PASSWORD=tu_contrase√±a

# Configuraci√≥n de Scraping
DOWNLOAD_FOLDER=downloads
LOG_LEVEL=INFO
5. Configurar base de datos
bash# Crear base de datos (opcional, se crea autom√°ticamente)
createdb books_scraper
üéØ Uso
Scraping Manual
Scraping Est√°tico (R√°pido)
bashpython scraper_static.py
Scraping Din√°mico (Completo con im√°genes)
bashpython scraper_dynamic.py
Programaci√≥n Autom√°tica
bashpython scheduler.py
El scheduler ejecutar√° ambos scrapers cada 30 minutos autom√°ticamente.
API REST
bashpython json_api_server.py
La API estar√° disponible en http://localhost:5000
Endpoints disponibles:
EndpointM√©todoDescripci√≥n/api/productosGETObtener todos los libros/api/archivosGETObtener archivos descargados
Ejemplo de respuesta:
json{
  "productos": [
    {
      "id": 1,
      "titulo": "A Light in the Attic",
      "precio": "¬£51.77",
      "url": "downloads/image_001.jpg",
      "fecha_registro": "2025-01-15T10:30:00"
    }
  ]
}
